{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32b84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T11:25:46.893404Z",
     "start_time": "2023-06-01T11:25:42.285776Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import sklearn.model_selection\n",
    "import os, re\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from keras.engine import base_layer_utils\n",
    "\n",
    "from keras import backend\n",
    "from keras import constraints\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras.engine.base_layer import Layer\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "##################\n",
    "# Verifications:\n",
    "#################\n",
    "print('GPU is used.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'GPU is NOT used.')\n",
    "print(\"Tensorflow version: \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d639d",
   "metadata": {},
   "source": [
    "# Load the IMDB movie review sentiment data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "zip_path = keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\", data_url, cache_subdir=\"datasets/aclImdb_v1/\", extract=True\n",
    ")\n",
    "\n",
    "train_folder_pos = os.path.dirname(zip_path) + \"/aclImdb/train/pos/\"\n",
    "train_folder_neg = os.path.dirname(zip_path) + \"/aclImdb/train/neg/\"\n",
    "test_folder_pos = os.path.dirname(zip_path) + \"/aclImdb/test/pos/\"\n",
    "test_folder_neg = os.path.dirname(zip_path) + \"/aclImdb/test/neg/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb8492",
   "metadata": {},
   "source": [
    "## Read IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces_to_punctuation(string):\n",
    "    pattern = r\"([^\\w\\s])\"\n",
    "    modified_string = re.sub(pattern, r\" \\1 \", string)\n",
    "    return modified_string\n",
    "\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    docs = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Perform desired operations on the file\n",
    "                file_contents = file.read()\n",
    "                # Do something with the file contents\n",
    "                file_contents = add_spaces_to_punctuation(file_contents)\n",
    "                docs.append(file_contents.lower())\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Specify the directory path you want to read files from\n",
    "\n",
    "# Call the function to read files in the repository\n",
    "train_pos = read_files_in_directory(train_folder_pos)\n",
    "train_neg = read_files_in_directory(train_folder_neg)\n",
    "test_pos = read_files_in_directory(test_folder_pos)\n",
    "test_neg = read_files_in_directory(test_folder_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1ffbb",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "We first prepare the vocabulary to be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ee901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary\n",
    "vocab_size = 20000\n",
    "vocab = {}\n",
    "for doc in train_pos + train_neg:\n",
    "    doc = doc.split(\" \")\n",
    "    for token in doc:\n",
    "        v = vocab.get(token, 0)\n",
    "        vocab[token] = v + 1\n",
    "\n",
    "\n",
    "vocab = list(dict(sorted(vocab.items(), key=lambda x: -x[1])[0:vocab_size]).keys())\n",
    "# Mapping tokens to integers\n",
    "token_to_num = keras.layers.StringLookup(\n",
    "    vocabulary=vocab, oov_token=\"[UNK]\", mask_token=\"[ZERO]\"\n",
    ")\n",
    "# Mapping integers back to original tokens\n",
    "num_to_token = keras.layers.StringLookup(\n",
    "    vocabulary=token_to_num.get_vocabulary(),\n",
    "    oov_token=\"[UNK]\",\n",
    "    mask_token=\"[ZERO]\",\n",
    "    invert=True,\n",
    ")\n",
    "vocab_size = token_to_num.vocabulary_size()\n",
    "print(f\"The size of the vocabulary ={token_to_num.vocabulary_size()}\")\n",
    "print(\"Top 20 tokens in the vocabulary: \", token_to_num.get_vocabulary()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97052b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_nums(docs):\n",
    "    out = []\n",
    "    for doc in tqdm(docs):\n",
    "        word_splits = tf.strings.split(doc, sep=\" \")\n",
    "        doc_list = token_to_num(word_splits)\n",
    "        out.append(doc_list)\n",
    "    return out\n",
    "\n",
    "\n",
    "x_train = docs_to_nums(train_pos + train_neg)\n",
    "x_test = docs_to_nums(test_pos + test_neg)\n",
    "\n",
    "y_train = np.array([1] * len(train_pos) + [0] * len(train_neg))\n",
    "y_test = np.array([1] * len(test_pos) + [0] * len(test_neg))\n",
    "x_val, x_test,y_val,y_test = sklearn.model_selection.train_test_split(x_test, y_test, train_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12611f54",
   "metadata": {},
   "source": [
    "# Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37e8368",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custome_Embedding(layers.Layer):\n",
    "    \"\"\"Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "    e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "    This layer can only be used on positive integer inputs of a fixed range. The\n",
    "    tf.keras.layers.TextVectorization, tf.keras.layers.StringLookup,\n",
    "    and tf.keras.layers.IntegerLookup preprocessing layers can help prepare\n",
    "    inputs for an Embedding layer.\n",
    "\n",
    "    This layer accepts tf.Tensor, tf.RaggedTensor and tf.SparseTensor\n",
    "    input.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> model = tf.keras.Sequential()\n",
    "    >>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
    "    >>> # The model will take as input an integer matrix of size (batch,\n",
    "    >>> # input_length), and the largest integer (i.e. word index) in the input\n",
    "    >>> # should be no larger than 999 (vocabulary size).\n",
    "    >>> # Now model.output_shape is (None, 10, 64), where None is the batch\n",
    "    >>> # dimension.\n",
    "    >>> input_array = np.random.randint(1000, size=(32, 10))\n",
    "    >>> model.compile('rmsprop', 'mse')\n",
    "    >>> output_array = model.predict(input_array)\n",
    "    >>> print(output_array.shape)\n",
    "    (32, 10, 64)\n",
    "\n",
    "    Args:\n",
    "      input_dim: Integer. Size of the vocabulary,\n",
    "        i.e. maximum integer index + 1.\n",
    "      output_dim: Integer. Dimension of the dense embedding.\n",
    "      embeddings_initializer: Initializer for the embeddings\n",
    "        matrix (see keras.initializers).\n",
    "      embeddings_regularizer: Regularizer function applied to\n",
    "        the embeddings matrix (see keras.regularizers).\n",
    "      embeddings_constraint: Constraint function applied to\n",
    "        the embeddings matrix (see keras.constraints).\n",
    "      mask_zero: Boolean, whether or not the input value 0 is a special\n",
    "        \"padding\" value that should be masked out. This is useful when using\n",
    "        recurrent layers which may take variable length input. If this is\n",
    "        True, then all subsequent layers in the model need to support masking\n",
    "        or an exception will be raised. If mask_zero is set to True, as a\n",
    "        consequence, index 0 cannot be used in the vocabulary (input_dim should\n",
    "        equal size of vocabulary + 1).\n",
    "      input_length: Length of input sequences, when it is constant.\n",
    "        This argument is required if you are going to connect\n",
    "        Flatten then Dense layers upstream\n",
    "        (without it, the shape of the dense outputs cannot be computed).\n",
    "      sparse: If True, calling this layer returns a tf.SparseTensor. If False,\n",
    "        the layer returns a dense tf.Tensor. For an entry with no features in\n",
    "        a sparse tensor (entry with value 0), the embedding vector of index 0 is\n",
    "        returned by default.\n",
    "\n",
    "    Input shape:\n",
    "      2D tensor with shape: (batch_size, input_length).\n",
    "\n",
    "    Output shape:\n",
    "      3D tensor with shape: (batch_size, input_length, output_dim).\n",
    "\n",
    "    *Note on variable placement:*\n",
    "    By default, if a GPU is available, the embedding matrix will be placed on\n",
    "    the GPU. This achieves the best performance, but it might cause issues:\n",
    "\n",
    "    - You may be using an optimizer that does not support sparse GPU kernels.\n",
    "    In this case you will see an error upon training your model.\n",
    "    - Your embedding matrix may be too large to fit on your GPU. In this case\n",
    "    you will see an Out Of Memory (OOM) error.\n",
    "\n",
    "    In such cases, you should place the embedding matrix on the CPU memory.\n",
    "    You can do so with a device scope, as such:\n",
    "\n",
    "    \n",
    "    with tf.device('cpu:0'):\n",
    "      embedding_layer = Embedding(...)\n",
    "      embedding_layer.build()\n",
    "    \n",
    "\n",
    "    The pre-built embedding_layer instance can then be added to a Sequential\n",
    "    model (e.g. model.add(embedding_layer)), called in a Functional model\n",
    "    (e.g. x = embedding_layer(x)), or used in a subclassed model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        output_dim,\n",
    "        embeddings_initializer=\"uniform\",\n",
    "        embeddings_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        embeddings_constraint=None,\n",
    "        mask_zero=False,\n",
    "        input_length=None,\n",
    "        sparse=False,\n",
    "        use_zero_vector=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if \"input_shape\" not in kwargs:\n",
    "            if input_length:\n",
    "                kwargs[\"input_shape\"] = (input_length,)\n",
    "            else:\n",
    "                kwargs[\"input_shape\"] = (None,)\n",
    "        if input_dim <= 0 or output_dim <= 0:\n",
    "            raise ValueError(\n",
    "                \"Both input_dim and output_dim should be positive, \"\n",
    "                f\"Received input_dim = {input_dim} \"\n",
    "                f\"and output_dim = {output_dim}\"\n",
    "            )\n",
    "        if (\n",
    "            not base_layer_utils.v2_dtype_behavior_enabled()\n",
    "            and \"dtype\" not in kwargs\n",
    "        ):\n",
    "            # In TF1, the dtype defaults to the input dtype which is typically\n",
    "            # int32, so explicitly set it to floatx\n",
    "            kwargs[\"dtype\"] = backend.floatx()\n",
    "        # We set autocast to False, as we do not want to cast floating- point\n",
    "        # inputs to self.dtype. In call(), we cast to int32, and casting to\n",
    "        # self.dtype before casting to int32 might cause the int32 values to be\n",
    "        # different due to a loss of precision.\n",
    "        kwargs[\"autocast\"] = False\n",
    "        use_one_hot_matmul = kwargs.pop(\"use_one_hot_matmul\", False)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings_initializer = initializers.get(embeddings_initializer)\n",
    "        self.embeddings_regularizer = regularizers.get(embeddings_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "        self.embeddings_constraint = constraints.get(embeddings_constraint)\n",
    "        self.mask_zero = mask_zero\n",
    "        self.supports_masking = mask_zero\n",
    "        self.input_length = input_length\n",
    "        self.sparse = sparse\n",
    "        self.use_zero_vector = use_zero_vector\n",
    "        if self.sparse and self.mask_zero:\n",
    "            raise ValueError(\n",
    "                \"mask_zero cannot be enabled when \"\n",
    "                \"tf.keras.layers.Embedding is used with tf.SparseTensor \"\n",
    "                \"input.\"\n",
    "            )\n",
    "        # Make this flag private and do not serialize it for now.\n",
    "        # It will be part of the public API after further testing.\n",
    "        self._use_one_hot_matmul = use_one_hot_matmul\n",
    "\n",
    "    def build(self, input_shape=None):\n",
    "        if self.use_zero_vector == True:\n",
    "            self.zero_vector = tf.constant(0.0, shape=(1, self.output_dim))        \n",
    "            self.embeddings = self.add_weight(\n",
    "                shape=(self.input_dim-1, self.output_dim),\n",
    "                initializer=self.embeddings_initializer,\n",
    "                name=\"embeddings\",\n",
    "                regularizer=self.embeddings_regularizer,\n",
    "                constraint=self.embeddings_constraint,\n",
    "                experimental_autocast=False,\n",
    "            )\n",
    "        else:\n",
    "            self.embeddings = self.add_weight(\n",
    "                shape=(self.input_dim, self.output_dim),\n",
    "                initializer=self.embeddings_initializer,\n",
    "                name=\"embeddings\",\n",
    "                regularizer=self.embeddings_regularizer,\n",
    "                constraint=self.embeddings_constraint,\n",
    "                experimental_autocast=False,\n",
    "            )\n",
    "        self.built = True\n",
    "\n",
    "    def get_embeddings(self):\n",
    "        if self.use_zero_vector == True:\n",
    "            return tf.concat([self.zero_vector, self.embeddings], axis=0).numpy()\n",
    "        else:\n",
    "            return self.embeddings.numpy()\n",
    "        \n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        if not self.mask_zero:\n",
    "            return None\n",
    "        return tf.not_equal(inputs, 0)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.input_length is None:\n",
    "            return input_shape + (self.output_dim,)\n",
    "        else:\n",
    "            # input_length can be tuple if input is 3D or higher\n",
    "            if isinstance(self.input_length, (list, tuple)):\n",
    "                in_lens = list(self.input_length)\n",
    "            else:\n",
    "                in_lens = [self.input_length]\n",
    "            if len(in_lens) != len(input_shape) - 1:\n",
    "                raise ValueError(\n",
    "                    f'\"input_length\" is {self.input_length}, but received '\n",
    "                    f\"input has shape {input_shape}\"\n",
    "                )\n",
    "            else:\n",
    "                for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n",
    "                    if s1 is not None and s2 is not None and s1 != s2:\n",
    "                        raise ValueError(\n",
    "                            f'\"input_length\" is {self.input_length}, but '\n",
    "                            f\"received input has shape {input_shape}\"\n",
    "                        )\n",
    "                    elif s1 is None:\n",
    "                        in_lens[i] = s2\n",
    "            return (input_shape[0],) + tuple(in_lens) + (self.output_dim,)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        if self.use_zero_vector == True:\n",
    "            full_embeddings = tf.concat([self.zero_vector, self.embeddings], axis=0)\n",
    "        else:\n",
    "            full_embeddings = self.embeddings       \n",
    "        \n",
    "        dtype = backend.dtype(inputs)\n",
    "        if dtype != \"int32\" and dtype != \"int64\":\n",
    "            inputs = tf.cast(inputs, \"int32\")\n",
    "        if isinstance(inputs, tf.sparse.SparseTensor):\n",
    "            if self.sparse:\n",
    "                # get sparse embedding values\n",
    "                embedding_values = tf.nn.embedding_lookup(\n",
    "                    params=full_embeddings, ids=inputs.values\n",
    "                )\n",
    "                embedding_values = tf.reshape(embedding_values, [-1])\n",
    "                # get sparse embedding indices\n",
    "                indices_values_embed_axis = tf.range(self.output_dim)\n",
    "                repeat_times = [inputs.indices.shape[0]]\n",
    "                indices_values_embed_axis = tf.expand_dims(\n",
    "                    tf.tile(indices_values_embed_axis, repeat_times), -1\n",
    "                )\n",
    "                indices_values_embed_axis = tf.cast(\n",
    "                    indices_values_embed_axis, dtype=tf.int64\n",
    "                )\n",
    "                current_indices = tf.repeat(\n",
    "                    inputs.indices, [self.output_dim], axis=0\n",
    "                )\n",
    "                new_indices = tf.concat(\n",
    "                    [current_indices, indices_values_embed_axis], 1\n",
    "                )\n",
    "                new_shape = tf.concat(\n",
    "                    [tf.cast(inputs.shape, dtype=tf.int64), [self.output_dim]],\n",
    "                    axis=-1,\n",
    "                )\n",
    "                out = tf.SparseTensor(\n",
    "                    indices=new_indices,\n",
    "                    values=embedding_values,\n",
    "                    dense_shape=new_shape,\n",
    "                )\n",
    "            else:\n",
    "                sparse_inputs_expanded = tf.sparse.expand_dims(inputs, axis=-1)\n",
    "                out = tf.nn.safe_embedding_lookup_sparse(\n",
    "                    embedding_weights=full_embeddings,\n",
    "                    sparse_ids=sparse_inputs_expanded,\n",
    "                    default_id=0,\n",
    "                )\n",
    "        elif self._use_one_hot_matmul:\n",
    "            # Note that we change the dtype of the one_hot to be same as the\n",
    "            # weight tensor, since the input data are usually ints, and weights\n",
    "            # are floats. The nn.embedding_lookup support ids as ints, but\n",
    "            # the one_hot matmul need both inputs and weights to be same dtype.\n",
    "            one_hot_data = tf.one_hot(\n",
    "                inputs, depth=self.input_dim, dtype=self.dtype\n",
    "            )\n",
    "            out = tf.matmul(one_hot_data, full_embeddings)\n",
    "        else:\n",
    "            out = tf.nn.embedding_lookup(full_embeddings, inputs)\n",
    "\n",
    "        if self.sparse and not isinstance(out, tf.SparseTensor):\n",
    "            out = tf.sparse.from_dense(out)\n",
    "\n",
    "        if (\n",
    "            self._dtype_policy.compute_dtype\n",
    "            != self._dtype_policy.variable_dtype\n",
    "        ):\n",
    "            # Instead of casting the variable as in most layers, cast the\n",
    "            # output, as this is mathematically equivalent but is faster.\n",
    "            out = tf.cast(out, self._dtype_policy.compute_dtype)\n",
    "        return out\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"embeddings_initializer\": initializers.serialize(\n",
    "                self.embeddings_initializer\n",
    "            ),\n",
    "            \"embeddings_regularizer\": regularizers.serialize(\n",
    "                self.embeddings_regularizer\n",
    "            ),\n",
    "            \"activity_regularizer\": regularizers.serialize(\n",
    "                self.activity_regularizer\n",
    "            ),\n",
    "            \"embeddings_constraint\": constraints.serialize(\n",
    "                self.embeddings_constraint\n",
    "            ),\n",
    "            \"mask_zero\": self.mask_zero,\n",
    "            \"input_length\": self.input_length,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65852208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, learning_rate, use_zero_vector):\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "    # Embed each integer in a 128-dimensional vector\n",
    "    x = Custome_Embedding(vocab_size, 128, use_zero_vector=use_zero_vector)(inputs)\n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = layers.LSTM(64, use_bias=False)(x)\n",
    "    # Add a classifier\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", use_bias=False)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_init(run):\n",
    "    # Start a run, tracking hyperparameters\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"padding_analysis\",\n",
    "        name=run,\n",
    "        # track hyperparameters and run metadata with wandb.config\n",
    "        config={\n",
    "            \"epoch\": 50,\n",
    "            \"batch_size\": 128,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"seq_max_len\": 500,\n",
    "            \"patience\": 10\n",
    "\n",
    "        }\n",
    "    )\n",
    "    # [optional] use wandb.config as your config\n",
    "    config = wandb.config\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4a09d",
   "metadata": {},
   "source": [
    "# Train and evaluate the model with post_padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2bef6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_post_padding = []\n",
    "for i in range(10):    \n",
    "    config = wandb_init(\"imdb_post_padding_\"+str(i))\n",
    "    x_train_padded = keras.utils.pad_sequences(x_train, maxlen=config.seq_max_len, padding=\"post\")\n",
    "    x_val_padded = keras.utils.pad_sequences(x_val, maxlen=config.seq_max_len, padding=\"post\")\n",
    "    x_test_padded = keras.utils.pad_sequences(x_test, maxlen=config.seq_max_len, padding=\"post\")\n",
    "    \n",
    "    model_post_padding = build_model(vocab_size, config.learning_rate, use_zero_vector=False)\n",
    "    # model_post_padding.summary()\n",
    "    # plot_model(model_post_padding, show_shapes=True)\n",
    "    model_post_padding.fit(\n",
    "        x_train_padded,\n",
    "        y_train,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epoch,\n",
    "        validation_data=(x_test_padded, y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=config.patience),\n",
    "            WandbMetricsLogger(log_freq=5),\n",
    "            WandbModelCheckpoint(\"models\")\n",
    "        ]\n",
    "    )\n",
    "    results_post_padding.append(model_post_padding.evaluate(\n",
    "        x_test_padded,\n",
    "        y_test,\n",
    "        batch_size=128\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2a1e47",
   "metadata": {},
   "source": [
    "# Train and evaluate the model with pre_padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pre_padding = []\n",
    "for i in range(10):\n",
    "    config = wandb_init(\"imdb_pre_padding\"+str(i))\n",
    "    x_train_padded = keras.utils.pad_sequences(x_train, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "    x_val_padded = keras.utils.pad_sequences(x_val, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "    x_test_padded = keras.utils.pad_sequences(x_test, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "    model_pre_padding = build_model(vocab_size, config.learning_rate, use_zero_vector=False)\n",
    "    # model_pre_padding.summary()\n",
    "    # plot_model(model_pre_padding, show_shapes=True)\n",
    "    model_pre_padding.fit(\n",
    "        x_train_padded,\n",
    "        y_train,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epoch,\n",
    "        validation_data=(x_test_padded, y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=config.patience),\n",
    "            WandbMetricsLogger(log_freq=5),\n",
    "            WandbModelCheckpoint(\"models\")\n",
    "        ]\n",
    "    )\n",
    "    results_pre_padding.append(model_pre_padding.evaluate(\n",
    "        x_test_padded,\n",
    "        y_test,\n",
    "        batch_size=128\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b96c2",
   "metadata": {},
   "source": [
    "# Train and evaluate the model with pre_padding ZERO Vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_pre_padding_zero_padding = []\n",
    "for i in range(10):\n",
    "    config = wandb_init(\"imdb_pre_padding_zero_vector\"+str(i))\n",
    "    x_train_padded = keras.utils.pad_sequences(x_train, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "    x_val_padded = keras.utils.pad_sequences(x_val, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "    x_test_padded = keras.utils.pad_sequences(x_test, maxlen=config.seq_max_len, padding=\"pre\")\n",
    "\n",
    "    model_pre_padding_zero_padding = build_model(vocab_size, config.learning_rate, use_zero_vector=False)\n",
    "    # model_pre_padding_zero_padding.summary()\n",
    "    # plot_model(model_pre_padding_zero_padding, show_shapes=True)\n",
    "    model_pre_padding_zero_padding.fit(\n",
    "        x_train_padded,\n",
    "        y_train,\n",
    "        batch_size=config.batch_size,\n",
    "        epochs=config.epoch,\n",
    "        validation_data=(x_test_padded, y_test),\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(restore_best_weights=True, patience=config.patience),\n",
    "            WandbMetricsLogger(log_freq=5),\n",
    "            WandbModelCheckpoint(\"models\")\n",
    "        ]\n",
    "    )\n",
    "    results_pre_padding_zero_padding.append(model_pre_padding_zero_padding.evaluate(\n",
    "        x_test_padded,\n",
    "        y_test,\n",
    "        batch_size=128\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5bc2f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('reda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05670586",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = open('/dev/stdout', 'w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6b438d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('results_post_padding: ' + str(np.mean([x[1] for x in results_post_padding])))\n",
    "print('results_pre_padding: ' + str(np.mean([x[1] for x in results_pre_padding])))\n",
    "print('results_pre_padding_zero_padding: ' + str(np.mean([x[1] for x in results_pre_padding_zero_padding])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa823ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([x[1] for x in results_pre_padding])\n",
    "print([x[1] for x in results_pre_padding_zero_padding])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf63a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
