{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a32b84d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T11:25:46.893404Z",
     "start_time": "2023-06-01T11:25:42.285776Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os, re\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "##################\n",
    "# Verifications:\n",
    "#################\n",
    "print('GPU is used.' if len(tf.config.list_physical_devices('GPU')) > 0 else 'GPU is NOT used.')\n",
    "print(\"Tensorflow version: \" + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d639d",
   "metadata": {},
   "source": [
    "# Load the IMDB movie review sentiment data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3d398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "zip_path = keras.utils.get_file(\n",
    "    \"aclImdb_v1.tar.gz\", data_url, cache_subdir=\"datasets/aclImdb_v1/\", extract=True\n",
    ")\n",
    "\n",
    "train_folder_pos = os.path.dirname(zip_path) + \"/aclImdb/train/pos/\"\n",
    "train_folder_neg = os.path.dirname(zip_path) + \"/aclImdb/train/neg/\"\n",
    "test_folder_pos = os.path.dirname(zip_path) + \"/aclImdb/test/pos/\"\n",
    "test_folder_neg = os.path.dirname(zip_path) + \"/aclImdb/test/neg/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb8492",
   "metadata": {},
   "source": [
    "## Read IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1161e80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_spaces_to_punctuation(string):\n",
    "    pattern = r\"([^\\w\\s])\"\n",
    "    modified_string = re.sub(pattern, r\" \\1 \", string)\n",
    "    return modified_string\n",
    "\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    docs = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                # Perform desired operations on the file\n",
    "                file_contents = file.read()\n",
    "                # Do something with the file contents\n",
    "                file_contents = add_spaces_to_punctuation(file_contents)\n",
    "                docs.append(file_contents.lower())\n",
    "    return docs\n",
    "\n",
    "\n",
    "# Specify the directory path you want to read files from\n",
    "\n",
    "# Call the function to read files in the repository\n",
    "train_pos = read_files_in_directory(train_folder_pos)\n",
    "train_neg = read_files_in_directory(train_folder_neg)\n",
    "test_pos = read_files_in_directory(test_folder_pos)\n",
    "test_neg = read_files_in_directory(test_folder_neg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1ffbb",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "We first prepare the vocabulary to be used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ee901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary\n",
    "vocab_size = 20000\n",
    "vocab = {}\n",
    "for doc in train_pos + train_neg:\n",
    "    doc = doc.split(\" \")\n",
    "    for token in doc:\n",
    "        v = vocab.get(token, 0)\n",
    "        vocab[token] = v + 1\n",
    "\n",
    "\n",
    "vocab = list(dict(sorted(vocab.items(), key=lambda x: -x[1])[0:vocab_size]).keys())\n",
    "# Mapping tokens to integers\n",
    "token_to_num = keras.layers.StringLookup(\n",
    "    vocabulary=vocab, oov_token=\"[UNK]\", mask_token=\"[ZERO]\"\n",
    ")\n",
    "# Mapping integers back to original tokens\n",
    "num_to_token = keras.layers.StringLookup(\n",
    "    vocabulary=token_to_num.get_vocabulary(),\n",
    "    oov_token=\"[UNK]\",\n",
    "    mask_token=\"[ZERO]\",\n",
    "    invert=True,\n",
    ")\n",
    "vocab_size = token_to_num.vocabulary_size()\n",
    "print(f\"The size of the vocabulary ={token_to_num.vocabulary_size()}\")\n",
    "print(\"Top 20 tokens in the vocabulary: \", token_to_num.get_vocabulary()[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97052b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_to_nums(docs):\n",
    "    out = []\n",
    "    for doc in tqdm(docs):\n",
    "        word_splits = tf.strings.split(doc, sep=\" \")\n",
    "        doc_list = token_to_num(word_splits)\n",
    "        out.append(doc_list)\n",
    "    return out\n",
    "\n",
    "\n",
    "x_train = docs_to_nums(train_pos + train_neg)\n",
    "x_test = docs_to_nums(test_pos + test_neg)\n",
    "\n",
    "y_train = np.array([1] * len(train_pos) + [0] * len(train_neg))\n",
    "y_test = np.array([1] * len(test_pos) + [0] * len(test_neg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12611f54",
   "metadata": {},
   "source": [
    "# Build the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65852208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size):\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
    "    # Embed each integer in a 128-dimensional vector\n",
    "    x = layers.Embedding(vocab_size, 128)(inputs)\n",
    "    # Add 2 bidirectional LSTMs\n",
    "    x = layers.LSTM(64, use_bias=False)(x)\n",
    "    # Add a classifier\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\", use_bias=False)(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4a09d",
   "metadata": {},
   "source": [
    "# Train and evaluate the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe2bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "x_train_padded = keras.utils.pad_sequences(x_train, maxlen=maxlen, padding=\"post\")\n",
    "x_test_padded = keras.utils.pad_sequences(x_test, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "model = build_model(vocab_size)\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c42e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "history1 = model.fit(\n",
    "    x_train_padded,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test_padded, y_test),\n",
    ")\n",
    "model.evaluate(\n",
    "    x_train_padded,\n",
    "    y_train,\n",
    "    batch_size=32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c991dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 500\n",
    "x_train_padded = keras.utils.pad_sequences(x_train, maxlen=maxlen, padding=\"pre\")\n",
    "x_test_padded = keras.utils.pad_sequences(x_test, maxlen=maxlen, padding=\"pre\")\n",
    "\n",
    "\n",
    "model = build_model(vocab_size)\n",
    "model.summary()\n",
    "plot_model(model, show_shapes=True)\n",
    "\n",
    "\n",
    "\n",
    "history2 = model.fit(\n",
    "    x_train_padded,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    validation_data=(x_test_padded, y_test),\n",
    ")\n",
    "\n",
    "model.evaluate(\n",
    "    x_train_padded,\n",
    "    y_train,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08deb237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_plots(history):\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    ax1 = plt.subplot(1, 2, 1)\n",
    "    for l in history.history:\n",
    "        if l == 'loss' or l == 'val_loss':\n",
    "            loss = history.history[l]\n",
    "            plt.plot(range(1, len(loss) + 1), loss, label=l)\n",
    "\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    for k in history.history:\n",
    "        if 'accuracy' in k:\n",
    "            loss = history.history[k]\n",
    "            plt.plot(range(1, len(loss) + 1), loss, label=k)\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca5cd07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
